#  3.2M FP32 transformer model
  
Well rn.. only pretraining is done via a tiny-shakespare dataset
Royal Gibberish is best. it can generate
