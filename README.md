#  3.2M FP32 transformer model
  
Well rn.. only pretraining is done via a tiny-shakespare dataset

Royal Gibberish is best. it can generate

interestingly i made another model on the same arch with 10.78M parameter was trained over the same dataset 
<img width="531" height="270" alt="image" src="https://github.com/user-attachments/assets/9a4b8004-b6b3-434e-9dbe-94c4e649c99f" />

clearly overfitting was observable 
well the text quality is gud thou..

<img width="601" height="430" alt="image" src="https://github.com/user-attachments/assets/0f01e238-da2f-42f3-974b-354a7acb3799" />


